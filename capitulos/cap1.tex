% !TEX root = ../main.tex
%!TeX spellcheck = es-CL

\documentclass[../main.tex]{subfiles}
\chapter{Modelos gráficos probabilísticos}

\begin{quotation}
    El \textit{machine learning} fue un objeto de lujo, pero para nosotros es
    un artículo de primera necesidad: no podemos vivir sin \textit{machine learning}.
	\NC{Poner algo profundo por el estilo. (?)}
\end{quotation}

\section*{Introducción}

El eje central de este capitulo se basa en la búsqueda de una representación compacta, para
distribuciones de probabilidad conjunta de la forma $p(\bm x | \bm\theta)$. Esto, con
la intención de realizar inferencia sobre variables y aprendizaje de parámetros de manera
eficiente


\section{Modelos gráficos dirigidos}
Toda distribución de probabilidad conjunta $p(\bm x ) = p(x_1, x_2, \ldots x_v)$  se puede
representar de la forma:
\begin{equation}
	\label{eq:regla_cadena}
	p(\bm x ) = p(x_1)p(x_2 \ \vline \ x_1 )p(x_3 \ \vline\  x_1, x_2) \ldots p(x_v\ \vline\  x_1, x_2, \ldots , x_{v-1})
\end{equation}

\NC{discusión sobre computabilidad. (?) (ref: Murphy, pg. 307)}
El problema con esta expresión es la dificultad computacional subyacente al cálculo de
distribuciones condicionales de la forma $p(x_{t} |  x_1,\ldots, x_{t-1} )$
cuando el número de variables incidentes $t$ aumenta.

\NC{propiedades básicas del calculo de probabilidades (CI por ej.) al apéndice.}
No obstante, la representación (\ref{eq:regla_cadena}) reduce su complejidad en presencia de
\index{independencia condicional} \textbf{independencia condicional}.

En efecto, si se asume $x_{t+1} \perp  x_{1}, \ldots, x_{t-1} \ \vline \ x_t$. Es decir,
las observaciones futuras $x_{t+1}$ son independientes del pasado $ x_{1}, \ldots,
x_{t-1}$, dado el estado presente $x_{t}$. La probabilidad conjunta se reduce entonces a:
\begin{equation}
\label{eq:markov_prop}
p(\bm x ) = p(x_1) \prod_{t = 2}^{v} p(x_t  |  x_1,\ldots , x_{t-1}) = p(x_1) \prod_{t = 2}^{v} p(x_t  | x_{t-1})
\end{equation}

De lo cual se obtiene una expresión más simple.

Modelar la independencia condicional entre las variables permite entonces reducir la
complejidad de representación para la distribución conjunta. En particular, la elección tomada
en (\ref{eq:markov_prop}) se conoce como \index{Propiedad de Markov} \textbf{propiedad de
Markov} de primer orden.
En un contexto general, las relaciones de independencia condicional entre variables aleatorias
de dimensión arbitraria, se modelan utilizando \index{Diagramas de
independencia}\textit{diagramas de independencia} o \index{Modelos gráficos}\textbf{modelos
gráficos}. Estos se valen de un grafo $G=(\mathcal{V}, \mathcal{E})$ \footnote{Conjunto
consistente de $\mathcal{V} = \lbrace{1 \ldots , V} \rbrace$ vértices (o nodos) y $\mathcal{E} =
\lbrace {(s,t): s,t \in \mathcal{V}} \rbrace$ aristas.} para
representar mediante nodos $v = 1, \ldots, \mathcal{V}$ las variables aleatorias del modelo,
mientras que la presencia o ausencia de aristas entre estos nodos, permite modelar las
relaciones de dependencia condicional subyacentes.
% Naive Bayes Classifier
\begin{figure}[ht]
\centering
\subfloat[]{\label{fig:grafo_ejemplo}\input{./figuras/grafo_ejemplo}}
\quad
\subfloat[]{\label{fig:naive_bayes} \input{./figuras/naive_bayes_graph}}
\caption{\subref{fig:grafo_ejemplo} Ejemplo de modelo gráfico dirigido.
\subref{fig:naive_bayes} Relaciones de dependencia condicional en el clasificador
 naive Bayes como un modelo gráfico dirigido, las variables aleatorias observadas
 se denotan por nodos grises.}
\end{figure}

Una \index{Red bayesiana} \textit{red bayesiana} o \index{modelo gráfico dirigido}\textbf{modelo
gráfico dirigido} es un modelo gráfico probabilístico, cuyo grafo subyacente es un \index{grafo
dirigido acíclico} \textbf{grafo dirigido acíclico} (\index{DAG}DAG por sus siglas en ingles).
Todo DAG posee un \index{ordenamiento topológico} \textit{ordenamiento topológico}, es decir,
los nodos de cualquier DAG pueden ser numerados de manera tal, que todo nodo padre
posea una numeración inferior a sus nodos hijos. \NC{lema de ordenamiento topológico
para DAG's en apéndice.} Esta característica permite enriquecer la formulación de la propiedad
de Markov (\ref{eq:markov_prop}), usando la estructura grafica como componente adicional. De
esta forma, se puede formular la \index{propiedad ordenada de Markov} \textbf{propiedad ordenada
de Markov} en modelos gráficos dirigidos:
\begin{equation}
	\label{eq:Markov_ordenado}
	x_s \perp \bm x_{pred(s) \setminus pa(s)} ~ | ~ \bm x_{pa(s)}
\end{equation}

Es decir, un nodo $x_s$ es independiente de aquellos predecesores, menores en orden topológico,
a sus padres $ \bm x_{pred(s) \setminus pa(s)}$, dados sus nodos padres $\bm x_{pa(s)}$. De
manera equivalente, un nodo $x_s$ solo depende de sus padres inmediatos $x_{pa(s)}$ y no de
todos sus predecesores.

De esta forma, la probabilidad conjunta de un modelo gráfico dirigido, que cumple la propiedad
ordenada de Markov, se puede descomponer de la forma:
\begin{equation}
\label{eq:markov_prop_ord}
p(\bm x ) = \prod_{t = 1}^{V} p(x_t  |  \bm x_{pa(t)})
\end{equation}

\begin{example}[Modelo grafico asociado a $p(\bm x)$]
Si se estudia un modelo probabilístico, donde la probabilidad conjunta de las variables
estudiadas $p(\bm x)$ esta dada por:
\begin{equation}
\label{eq:conj_grafo_ejemplo}
p(\bm x) = p(x_1)p(x_2)p(x_3)p(x_4 | x_1, x_2, x_3)p(x_5 | x_1, x_3)p(x_6 | x_4, x_5)
\end{equation}

Entonces, un grafo dirigido asociado a tal factorización es el de la figura
\ref{fig:grafo_ejemplo}. Para construir dicho grafo, se consideran las relaciones de
independencia condicional en la factorización (\ref{eq:conj_grafo_ejemplo}), para luego
establecer aristas $s \rightarrow t$ si la probabilidad condicional del nodo $x_s$ depende de
$x_t$. En este caso, no hay aristas incidentes hacia $x_1$, $x_2$ ni $x_3$. Por otra parte, se
deben crear aristas desde $x_1, x_2$ y $x_3$ hacia $x_4$, desde $x_1$ y $x_3$ hacia $x_5$ y
desde $x_4, x_5$ hacia $x_6$.

En general, es posible reconstruir la probabilidad conjunta subyacente a un modelo
gráfico probabilístico conociendo el grafo y haciendo el proceso inverso al descrito
anteriormente.
\end{example}

Con el fin de explorar las posibilidades de este tipo de modelos e introducir conceptos
 referentes a la notación de estos, se pasan a estudiar los siguientes ejemplos:

\subsection{Naive Bayes}
\index{Naive Bayes}

Dado un problema de clasificación de vectores $\bm x = (x_1, \ldots , x_V)$ en $C$ clases. Es
posible modelar las variables de decisión $x_t$ como condicionalmente independientes dada la
categoría de clasificación:
\begin{equation}
	\label{eq:cond_nb}
	x_i \perp x_j ~| y =  c, ~~ i \neq j
\end{equation}

Si se usa este enfoque, se obtiene que la densidad condicional de clases toma la forma:
\begin{equation}
	\label{eq:naive_bayes}
	p(\bm x ~ | y = c) = \prod_{t=1}^{V}p(x_t ~ | y = c)
\end{equation}

Al parametrizar las distribuciones de densidad condicional, es posible obtener un modelo de
clasificación conocido como \index{clasificador naive Bayes}\textbf{clasificador naive Bayes}.
 La estructura de las relaciones de independencia inducidas por (\ref{eq:cond_nb}) se pueden
expresar según (\ref{eq:naive_bayes}) y el modelo gráfico dirigido de la figura \ref{fig:naive_bayes}.

\subsection{Regresión polinomial}
\NC{añadir intro significativa}
las variables aleatorias son el vector de coeficientes polinomiales $\bm w$ y los
datos observados $\bm y = (y_1 , \ldots , y_N)^{T}$. Adicionalmente, se parametriza
el ruido del modelo a través de $\sigma_{\varepsilon}^2$ y la varianza de la distribución
a priori \footnote{Considerándose una distribución a priori, gaussiana y esférica sobre $\bm w$.}
de $\bm w$ por $\sigma_w^2$. Finalmente, los datos de entrada se denotan por $\bm x = (x_1 , \ldots , x_N)^{T}$.

La probabilidad conjunta de este modelo es el producto de la probabilidad
a priori $p(\bm w)$ con las distribuciones condicionales $p(y_i  |  \bm w)$
para $i = 1, \ldots, N$:
\begin{equation}
    \label{eq:poly_fact_1}
 p(\bm y, \bm w ) = p(\bm w) \prod_{i = 1}^{N} p(y_i  | \bm w)
\end{equation}

El grafo de tal factorización es similar al del clasificador naive Bayes \ref{fig:naive_bayes}.
Para representarlo de manera compacta, se usa la notación de de placas o \textit{plates},
en la figura \ref{fig:plate_poly_graph} se muestra el grafo de (\ref{eq:poly_fact_1})
usando esta convención. Aquí $N$ es la cantidad de nodos del modelo, de los cuales se muestra
el representante $y_i$.

\begin{figure}[ht]
\subfloat[]{\label{fig:plate_poly_graph}\input{./figuras/plate_poly_graph}}
\quad
\subfloat[]{\label{fig:plate_poly_graph_2}\input{./figuras/plate_poly_graph_2}}
\quad
\subfloat[]{\label{fig:plate_poly_graph_final}\input{./figuras/plate_poly_graph_final}}
\caption{ Modelo grafico dirigido para regresión
polinomial usando notación de placas (o \textit{plates}). En \subref{fig:plate_poly_graph}
se muestra el grafo correspondiente a (\ref{eq:poly_fact_1}). En \subref{fig:plate_poly_graph_2}
se añaden los parámetros deterministas y las variables aleatorias observadas. En \subref{fig:plate_poly_graph_final} se añaden datos de entrada y predicciones.}
\end{figure}

Si por otra parte, si se quiere estudiar la interacción de los parámetros en el modelo,
es posible explicitarlos en la probabilidad conjunta para luego agregarlos al grafo:
\begin{equation}
\label{eq:poly_fact_2}
p(\bm y, \bm w | \bm x, \sigma_\varepsilon^2, \sigma_w^2 ) =
p(\bm w | \sigma_w^2) \prod_{i = 1}^{N} p(y_i | \bm w, x_i, \sigma_\varepsilon^2)
\end{equation}

La figura \ref{fig:plate_poly_graph_2} muestra el grafo correspondiente a (\ref{eq:poly_fact_2}).
Por convención, las variables deterministas se incluyen en el grafo como círculos
pequeños, mientras que las variables aleatorias observadas se muestran como nodos grises,
los nodos incoloros representan variables latentes o no observadas, finalmente las
aristas, al igual que en los ejemplos anteriores, representan la dependencia condicional
en la factorización de la probabilidad conjunta.
\NC{ver si es necesario cambiar el formato de los 3 grafos juntos. (muy pegados ?)}

Para realizar predicciones en datos nuevos $\hat{x}$, se desea encontrar la distribución
de probabilidad para $\hat{y}$ condicionada a la información que ya se posee. Esta
corresponde a:
\begin{equation}
    \label{eq:ploy_conj_pred_0}
    p(\hat{y}, \bm y, \bm w | \hat{x}, \bm x, \sigma_w^2, \sigma_\varepsilon^2)
    = \left[ \prod_{i=1}^{N}p(y_i | x_i, \bm w, \sigma_\varepsilon^2) \right]
    p(\bm w | \sigma_w^2) p(\hat{y} | \hat{x}, \bm w, \sigma_\varepsilon^2)
\end{equation}

Finalmente, se deduce la distribución predictiva para $\hat{y}$ :
\begin{equation}
    \label{eq:ploy_pred}
    p(\hat{y}| \hat{x}, \bm y, \bm x, \sigma_w^2, \sigma_\varepsilon^2)
    \propto \int p(\hat{y}, \bm y, \bm w | \hat{x}, \bm x,  \sigma_w^2,
    \sigma_\varepsilon^2) ~ d \bm w
\end{equation}

El modelo gráfico dirigido que encapsula estas últimas ecuaciones se aprecia en
\ref{fig:plate_poly_graph_final}.

\subsection{Modelos gráficos dirigidos gaussianos}

Sea $\mathcal{M}$ un modelo grafico dirigido, en el cual todas las variables son
reales y sus distribuciones de probabilidad condicional son lineal-gaussianas:
\begin{equation}
	\label{eq:gaussian_cpd}
	p(x_t  |  \bm x_{pa(t)}) = \mathcal{N}(x_t  |  \mu_t + \bm w_{t}^T \bm x_{pa(t)}, \sigma_t^2)
\end{equation}

La estructura de $\mathcal{M}$ permite modelar la probabilidad conjunta de las
variables del modelo en la forma:
\begin{equation}
	\label{eq:ver_red_bayesiana_gaussiana}
	p(\bm x  | \mathcal{M}) = \prod_{t=1}^{V} p(x_t  |  \bm x_{pa(t)}) =  \mathcal{N}(\bm x  |  \bm \mu , \Sigma)
\end{equation}

Lo cual se conoce como \index{red bayesiana gaussiana} \textbf{red bayesiana gaussiana}.
Para este tipo de modelos, es posible inferir
$\bm \mu$ y $ \bm \Sigma$. En efecto, según (\ref{eq:ver_red_bayesiana_gaussiana}):
\begin{equation}
	\label{eq:log_gaussian_cpd}
	 \log p( \bm x  |  \mathcal{M}) = - \sum_{t = 1}^{V} \frac{1}{2\sigma_t^2} \left(
	 x_t - \sum_{s \in pa(t)} w_{ts}  x_s - \mu_t	 \right)^2 + K
\end{equation}

Donde $K$ representa una constante independiente de de $\bm x$. Al ser la log-probabilidad
conjunta, cuadrática en las componentes de $\bm x$, se obtiene que efectivamente la
probabilidad conjunta es normal multivariada para  $\bm x$ en
(\ref{eq:ver_red_bayesiana_gaussiana}). Para estimar la media, se observa en primera instancia:
\begin{equation}
	\label{eq:E_x}
	x_t = \sum_{s \in pa(t)} w_{ts} \mathbb{E}[x_s] + \mu_t + \sigma_t \varepsilon_t
\end{equation}

Donde $\varepsilon_t \sim \mathcal{N}(0,1)$ y $\mathbb{E}[\varepsilon_t, \varepsilon_s] =  0 $,
para $s \neq t$. De esto se deduce:
\begin{equation}
\label{eq:E_x_2}
\mathbb{E}[x_t] = \sum_{s \in pa(t)} w_{ts} \mathbb{E}[x_s] + \mu_t
\end{equation}

Es posible entonces, encontrar las componentes de $\bm \mu =\mathbb{E}[\bm x]
=(\mathbb{E}[x_1], \ldots, \mathbb{E}[x_V])^T$ utilizando la estructura gráfica
dirigida de $\mathcal{M}$ (y por tanto su ordenamiento topológico). Para ello, se
comienza calculando $\mathbb{E}[x_1]$ para luego
continuar de manera recursiva según la numeración de los nodos.

Similarmente, es posible calcular el elemento $\bm \Sigma_{st}$ de la matriz de
covarianza, observando:
\begin{align}
    \label{eq:E_cov}
    \text{cov}(x_s,x_t) &= \mathbb{E}[(x_s - \mathbb{E}[x_s])(x_t - \mathbb{E}[x_t])] \nonumber \\
                        &= \left[ (x_s - \mathbb{E}[x_s]) \left\lbrace {\sum_{k \in pa(x_t)} w_{tk} (x_k - \mathbb{E}[x_k]) + \sigma_t \varepsilon_t} \right\rbrace \right]\\
                        &= \sum_{k \in pa(x_t)} w_{tk} ~ \text{cov}[x_s, x_t] + \sigma_t^2 \bm I_{st} \nonumber
\end{align}

De donde al igual que en (\ref{eq:E_x_2}), se calculan los elementos de $\bm \Sigma$
recursivamente.

Finalmente, se puede extender el modelo inducido por (\ref{eq:gaussian_cpd}) a uno
donde los nodos del modelo gráfico representen variables aleatorias gaussianas
multivariantes. Para esto, se reescribe la distribución de probabilidad condicional
para el nodo $x_t$ en la forma:
\begin{equation}
    \label{eq:gaussian_cpd_multi}
	p(\bm x_t ~ | ~ pa(\bm x_t)) =  \mathcal{N}\left( \bm x_t ~  \vline height 4ex
    depth 10pt \sum_{s \in pa(\bm x_t)} \bm W_{ts} \bm x_s + \bm \mu_t~, \bm \Sigma_t \right)
\end{equation}

Donde $\bm W_{ts}$ es una matriz de pesos entre los vectores de cada nodo.

\section{Independencia condicional en modelos gráficos dirigidos}

Como se mencionó anteriormente, los modelos gráficos encapsulan las relaciones
de independencia condicional entre las variables aleatorias del fenómeno que
se modela. En esta sección se estudian las propiedades de los modelos gráficos
dirigidos en cuanto sus propiedades \NC{terminar intro}

En un grafo $G$, se escribe $x_i \perp_{G} x_j | x_k$ si el nodo
$x_i$ es independiente de $x_j$ dado $x_k$. Se denota por $I(G)$ al conjunto
de todas las relaciones de independencia condicional codificadas en el grafo $G$.

\begin{definition}[Diagrama de independencia]
\label{def:diagrama_independencia}
Sea $p(\cdot)$ una distribución de probabilidad. Se dice que un grafo $G$ es un
diagrama de independencia o \textit{I-map}  para $p$ si y solo si $I(G) \subseteq I(p)$.
Donde $I(p)$ es el conjunto de todas las relaciones de independencia condicional
ciertas para $p$.
\end{definition}

De la definición anterior, se deduce que un grafo $G$ es un diagrama de independencia
para la distribución de probabilidad $p$, si este no contiene más relaciones de
independencia condicional que las permitidas por $p$. De esta forma, toda distribución
de probabilidad $p(\bm x)$, donde $\bm x = (x_1, ..., x_V)^T$, posee al menos un
diagrama de independencia. En efecto, si se considera un grafo $G$ con nodos
$\mathcal{V} = \lbrace{x_1, ..., x_V}\rbrace$ completamente conectados, entonces
$G$ es un diagrama de independencia para $p$ pues no presenta aristas faltantes.

De la discusión anterior, tiene sentido hablar de un \textit{diagrama de independencia minimal}
$G$ para $p$, es decir, un grafo $G$, tal que si $G'$ es otro diagrama de independencia
para $p$ que cumple  $G' \subseteq G$, entonces $G' = G$.

A continuación se estudian las características de los modelos gráficos dirigidos
que permiten determinar cuando existe independencia condicional en sus nodos.

\subsection{d-separación}

Se dice que un \textit{camino no dirigido} $P$ esta \textit{separado de manera dirigida}
o \textit{d-separado} por un conjunto de nodos $E$, si y solo si, se cumple alguna
de las siguiente condiciones:
\begin{enumerate}
    \item $P$ contiene una cadena, $s \rightarrow e \rightarrow t$, donde $e \in E$.
    \item $P$ contiene una estructura $s \leftarrow e \rightarrow t$, donde $e \in E$.
    \item $P$ contiene una estructura $s \rightarrow e \leftarrow t$, donde $e \notin E$
    o $e$ \textbf{no} es descendiente de algún elemento de $E$.
\end{enumerate}

Se dice que un conjunto de nodos $A$ esta d-separado de un conjunto de nodos $B$,
dado un conjunto de nodos $E$, si y solo si, todo camino no dirigido desde cada nodo
de $A$ a cada nodo de $B$ esta d-separado por $E$.

En un grafo acíclico dirigido $G$, se aprecia la siguiente propiedad:
\begin{flalign}
\bm x_A \perp_G \bm x_B | \bm x_E \iff A \text{ esta d-separado de } $B$ \text{ dado } E
\end{flalign}

Para comprender las propiedades anteriores, se analizan los siguientes ejemplos:

\begin{itemize}
    \item Sea $x \rightarrow y \rightarrow z$ una cadena, tal grafo codifica la
    siguiente probabilidad conjunta:
    \begin{equation*}
        p(x,y,z) = p(x) p(y | x)p(z | y)
    \end{equation*}
    Usando la propiedad (1), se puede deducir que $x \perp z | y$. Esto se comprueba
    pues:
    \begin{equation*}
    p(x,z | y) = \frac{p(x, y, z)}{p(y)} = \frac{p(x) p(y | x) p(z | y)}{p(y)}
    = \frac{p(x,y) p(z | y)}{p(y)}= p(x|y)p(z|y)
    \end{equation*}

    \item Sea la estructura $x \leftarrow y \rightarrow z$, según la propiedad (2),
    $x \perp z | y$. En efecto,
    \begin{equation*}
    p(x, z | y) = \frac{p(x, y, z)}{p(y)} = \frac{p(y)p(x|y)p(z|y)}{p(y)} =p(x|y)p(z|y)
    \end{equation*}

    \item Sea finalmente la estructura $x \rightarrow y \leftarrow z$, en este caso
    $x \not\perp z | y$ :
    \begin{equation*}
    p(x, z | y) = \frac{p(x, y, z)}{p(y)} = \frac{p(x)p(y|x,z)p(z)}{p(y)} \neq p(x|y)p(z|y)
    \end{equation*}
    Es por tal motivo que en la propiedad (3), se requiera en este tipo de estructuras,
    que no existan nodos, ni descendientes de nodos de la familia condicionante $E$.
\end{itemize}

Si bien en el último caso, no se cumplia $x \perp z | y$, si se tiene $p(x,z) = p(x)p(z)$,
es decir, $x$ y $z$ son marginalmente independientes, sin embargo, al condicionar ambos nodos
por un nodo hijo $y$, se vuelven dependientes, este efecto se denomina \textbf{paradoja de Berkson}.

\section{Modelos gráficos no dirigidos}
\section{Inferencia exacta en modelos gráficos}
